{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd369ebb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-28T17:18:09.153163Z",
     "iopub.status.busy": "2025-04-28T17:18:09.152600Z",
     "iopub.status.idle": "2025-04-28T17:37:05.424608Z",
     "shell.execute_reply": "2025-04-28T17:37:05.423168Z"
    },
    "papermill": {
     "duration": 1136.283137,
     "end_time": "2025-04-28T17:37:05.431155",
     "exception": false,
     "start_time": "2025-04-28T17:18:09.148018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:12<00:00, 13.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Epoch [1/60] - Time: 77.00s\n",
      "Train Loss: 1.5996 | Train Acc: 41.83%\n",
      "Val   Loss: 1.3667 | Val   Acc: 51.24%\n",
      "Saved Best Model!\n",
      "Epoch [2/60] - Time: 77.95s\n",
      "Train Loss: 1.1842 | Train Acc: 57.57%\n",
      "Val   Loss: 1.1992 | Val   Acc: 57.00%\n",
      "Saved Best Model!\n",
      "Epoch [3/60] - Time: 77.31s\n",
      "Train Loss: 1.0341 | Train Acc: 63.22%\n",
      "Val   Loss: 1.2161 | Val   Acc: 59.54%\n",
      "Saved Best Model!\n",
      "Epoch [4/60] - Time: 77.53s\n",
      "Train Loss: 0.9474 | Train Acc: 66.64%\n",
      "Val   Loss: 0.9155 | Val   Acc: 67.56%\n",
      "Saved Best Model!\n",
      "Epoch [5/60] - Time: 79.57s\n",
      "Train Loss: 0.8769 | Train Acc: 69.12%\n",
      "Val   Loss: 0.8479 | Val   Acc: 69.68%\n",
      "Saved Best Model!\n",
      "Epoch [6/60] - Time: 85.35s\n",
      "Train Loss: 0.7747 | Train Acc: 73.00%\n",
      "Val   Loss: 0.7718 | Val   Acc: 72.10%\n",
      "Saved Best Model!\n",
      "Epoch [7/60] - Time: 78.88s\n",
      "Train Loss: 0.7324 | Train Acc: 74.50%\n",
      "Val   Loss: 0.7579 | Val   Acc: 73.52%\n",
      "Saved Best Model!\n",
      "Epoch [8/60] - Time: 77.99s\n",
      "Train Loss: 0.7115 | Train Acc: 75.24%\n",
      "Val   Loss: 0.7111 | Val   Acc: 74.52%\n",
      "Saved Best Model!\n",
      "Epoch [9/60] - Time: 78.38s\n",
      "Train Loss: 0.6922 | Train Acc: 75.97%\n",
      "Val   Loss: 0.6900 | Val   Acc: 75.66%\n",
      "Saved Best Model!\n",
      "Epoch [10/60] - Time: 79.11s\n",
      "Train Loss: 0.6729 | Train Acc: 76.48%\n",
      "Val   Loss: 0.6937 | Val   Acc: 75.76%\n",
      "Saved Best Model!\n",
      "Epoch [11/60] - Time: 78.95s\n",
      "Train Loss: 0.6096 | Train Acc: 78.86%\n",
      "Val   Loss: 0.5959 | Val   Acc: 79.08%\n",
      "Saved Best Model!\n",
      "Epoch [12/60] - Time: 78.74s\n",
      "Train Loss: 0.5884 | Train Acc: 79.70%\n",
      "Val   Loss: 0.6366 | Val   Acc: 77.42%\n",
      "Epoch [13/60] - Time: 79.08s\n",
      "Train Loss: 0.5813 | Train Acc: 79.74%\n",
      "Val   Loss: 0.6282 | Val   Acc: 77.78%\n",
      "Epoch [14/60] - Time: 79.05s\n",
      "Train Loss: 0.5735 | Train Acc: 80.13%\n",
      "Val   Loss: 0.5901 | Val   Acc: 78.72%\n",
      "Early stopping after 14 epochs\n",
      "Total training time: 0h 18m 42s\n"
     ]
    }
   ],
   "source": [
    "#train.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ======================\n",
    "# Simplified CNN Model\n",
    "# ======================\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional block - reduced filters\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Second convolutional block - reduced filters\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Third convolutional block - reduced filters\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Simplified fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ======================\n",
    "# Training function\n",
    "# ======================\n",
    "def train(model, loader, optimizer, criterion, device, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "        \n",
    "    train_loss = running_loss / len(loader)\n",
    "    train_acc = 100. * correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "# ======================\n",
    "# Validation function\n",
    "# ======================\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "    val_loss = running_loss / len(loader)\n",
    "    val_acc = 100. * correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set device - prioritize GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "\n",
    "    # Define transforms - basic transform for faster processing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    # Augmentation only for training\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    # Load datasets with different transforms\n",
    "    train_val_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=train_transform)\n",
    "    \n",
    "    # Use less validation data for faster evaluation\n",
    "    val_size = 5000  # Fixed validation size\n",
    "    train_size = len(train_val_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "    # Increase batch size for faster training\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, \n",
    "                             num_workers=4, pin_memory=True if torch.cuda.is_available() else False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, \n",
    "                           num_workers=2, pin_memory=True if torch.cuda.is_available() else False)\n",
    "\n",
    "    # Initialize model\n",
    "    model = MyCNN().to(device)\n",
    "    \n",
    "    # Use mixed precision training if available (for newer GPUs)\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    # Learning rate scheduler - reduce learning rate when plateauing\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    # Reduce number of epochs\n",
    "    num_epochs = 60\n",
    "    best_val_acc = 0\n",
    "    patience = 3\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, scheduler)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Time: {epoch_time:.2f}s\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print('Saved Best Model!')\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs\")\n",
    "            break\n",
    "            \n",
    "        # Check if total training time is approaching 4 hours\n",
    "        total_time = time.time() - start_time\n",
    "        hours = total_time / 3600\n",
    "        if hours > 3.5:  # Stop if approaching 4 hours\n",
    "            print(f\"Training time limit approaching ({hours:.2f} hours). Stopping training.\")\n",
    "            break\n",
    "            \n",
    "    total_time = time.time() - start_time\n",
    "    hours, remainder = divmod(total_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f\"Total training time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35477211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T17:37:05.445551Z",
     "iopub.status.busy": "2025-04-28T17:37:05.444909Z",
     "iopub.status.idle": "2025-04-28T17:37:15.298020Z",
     "shell.execute_reply": "2025-04-28T17:37:15.296521Z"
    },
    "papermill": {
     "duration": 9.862349,
     "end_time": "2025-04-28T17:37:15.299772",
     "exception": false,
     "start_time": "2025-04-28T17:37:05.437423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Files already downloaded and verified\n",
      "Successfully loaded model from 'best_model.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14/2296061875.py:127: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/kaggle/input/model-v4-1/best_model_v4_1.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.49%\n",
      "Inference Time: 8.90 seconds\n",
      "\n",
      "Per-class accuracy:\n",
      "plane: 87.30%\n",
      "car: 92.10%\n",
      "bird: 74.60%\n",
      "cat: 69.40%\n",
      "deer: 86.40%\n",
      "dog: 77.40%\n",
      "frog: 88.60%\n",
      "horse: 86.40%\n",
      "ship: 91.80%\n",
      "truck: 90.90%\n"
     ]
    }
   ],
   "source": [
    "#test.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# ======================\n",
    "# Simplified CNN Model - Same as in train.py\n",
    "# ======================\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional block - reduced filters\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Second convolutional block - reduced filters\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Third convolutional block - reduced filters\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Simplified fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ======================\n",
    "# Test function\n",
    "# ======================\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = [0] * 10\n",
    "    class_total = [0] * 10\n",
    "    \n",
    "    # Time tracking\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = predicted[i]\n",
    "                if label == pred:\n",
    "                    class_correct[label] += 1\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    # Compute inference time\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    return test_acc, class_correct, class_total, inference_time\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "\n",
    "    # Define transforms - same normalization as training but no augmentation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    # Load test dataset\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    # Class names for reporting\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    # DataLoader with larger batch size for faster inference\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=512, \n",
    "        shuffle=False, \n",
    "        num_workers=4, \n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MyCNN().to(device)\n",
    "    \n",
    "    # Load best model\n",
    "    try:\n",
    "        model.load_state_dict(torch.load('/kaggle/input/model-v4-1/best_model_v4_1.pth'))\n",
    "        print(\"Successfully loaded model from 'best_model.pth'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Run test\n",
    "    test_acc, class_correct, class_total, inference_time = test(model, test_loader, device)\n",
    "    \n",
    "    # Print overall accuracy\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Inference Time: {inference_time:.2f} seconds\")\n",
    "    \n",
    "    # Print per-class accuracy\n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for i in range(10):\n",
    "        class_acc = 100 * class_correct[i] / class_total[i]\n",
    "        print(f'{classes[i]}: {class_acc:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c25f58",
   "metadata": {
    "papermill": {
     "duration": 0.007225,
     "end_time": "2025-04-28T17:37:15.313430",
     "exception": false,
     "start_time": "2025-04-28T17:37:15.306205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7277481,
     "sourceId": 11603311,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1155.256973,
   "end_time": "2025-04-28T17:37:18.775435",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-28T17:18:03.518462",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
